{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38e3cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import documentai  # type: ignore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea28642",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the environment variable\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/mohit/Documents/gen-lang-client-xxx.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefe7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"\"\n",
    "location = \"us\" # Format is \"us\" or \"eu\"\n",
    "processor_id = \"\" # Create processor before running sample\n",
    "file_path = \"statements/test_statement.pdf\"\n",
    "mime_type = \"application/pdf\" # Refer to https://cloud.google.com/document-ai/docs/file-types for supported file types\n",
    "# field_mask = \"text,entities,pages.pageNumber\"  # Optional. The fields to return in the Document object.\n",
    "processor_version_id = \"pretrained-bankstatement-v3.0-2022-05-16\" # Optional. Processor version to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a135e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse staement using DocumentAI\n",
    "def process_document_sample(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    file_path: str,\n",
    "    mime_type: str,\n",
    "    field_mask: Optional[str] = None,\n",
    "    processor_version_id: Optional[str] = None,\n",
    ") -> None:\n",
    "    # You must set the `api_endpoint` if you use a location other than \"us\".\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "    if processor_version_id:\n",
    "        # The full resource name of the processor version, e.g.:\n",
    "        # `projects/{project_id}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}`\n",
    "        name = client.processor_version_path(\n",
    "            project_id, location, processor_id, processor_version_id\n",
    "        )\n",
    "    else:\n",
    "        # The full resource name of the processor, e.g.:\n",
    "        # `projects/{project_id}/locations/{location}/processors/{processor_id}`\n",
    "        name = client.processor_path(project_id, location, processor_id)\n",
    "\n",
    "    # Read the file into memory\n",
    "    with open(file_path, \"rb\") as image:\n",
    "        image_content = image.read()\n",
    "\n",
    "    # Load binary data\n",
    "    raw_document = documentai.RawDocument(content=image_content, mime_type=mime_type)\n",
    "\n",
    "    # For more information: https://cloud.google.com/document-ai/docs/reference/rest/v1/ProcessOptions\n",
    "    # Optional: Additional configurations for processing.\n",
    "    process_options = None\n",
    "\n",
    "    request_params = {\n",
    "    \"name\": name,\n",
    "    \"raw_document\": raw_document,\n",
    "    \"field_mask\": field_mask,\n",
    "    }\n",
    "\n",
    "    if process_options:\n",
    "        request_params[\"process_options\"] = process_options\n",
    "\n",
    "    request = documentai.ProcessRequest(**request_params)\n",
    "\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "    # For a full list of `Document` object attributes, reference this page:\n",
    "    # https://cloud.google.com/document-ai/docs/reference/rest/v1/Document\n",
    "    document = result.document\n",
    "\n",
    "    # Read the text recognition output from the processor\n",
    "    print(\"The document contains the following text:\")\n",
    "    print(document.text)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "319206bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results in specific format\n",
    "def extract_entity_text(entity: documentai.Document.Entity, document_text: str) -> str:\n",
    "    \"\"\"Extract text from entity text segments\"\"\"\n",
    "    if not entity.text_anchor or not entity.text_anchor.text_segments:\n",
    "        return entity.mention_text or \"\"\n",
    "    \n",
    "    text_parts = []\n",
    "    for segment in entity.text_anchor.text_segments:\n",
    "        start_index = int(segment.start_index) if segment.start_index else 0\n",
    "        end_index = int(segment.end_index) if segment.end_index else len(document_text)\n",
    "        text_parts.append(document_text[start_index:end_index])\n",
    "    \n",
    "    return \"\".join(text_parts).strip()\n",
    "\n",
    "def parse_bank_statement(document: documentai.Document) -> Dict[str, Any]:\n",
    "    \"\"\"Parse bank statement into structured data with corrected logic.\"\"\"\n",
    "    \n",
    "    # Group entities by type\n",
    "    entities_by_type = {}\n",
    "    for entity in document.entities:\n",
    "        entity_type = entity.type_\n",
    "        if entity_type not in entities_by_type:\n",
    "            entities_by_type[entity_type] = []\n",
    "        entities_by_type[entity_type].append(entity)\n",
    "    \n",
    "    statement_info = {}\n",
    "\n",
    "    # --- CORRECTED & COMBINED HEURISTIC FOR BANK NAME ---\n",
    "    KNOWN_BANK_NAMES = [\"citi\", \"capital one\", \"chase\", \"bank of america\", \"discover\", \"wells fargo\", \"american express\"]\n",
    "    \n",
    "    bank_name_entity = entities_by_type.get('bank_name', [None])[0]\n",
    "    \n",
    "    # First, check if the model found a bank_name entity at all\n",
    "    if bank_name_entity:\n",
    "        extracted_bank_name = extract_entity_text(bank_name_entity, document.text)\n",
    "        \n",
    "        # Next, check if the extracted name seems incorrect (e.g., it's your name)\n",
    "        if \"MOHIT AGGARWAL\" in extracted_bank_name:\n",
    "            # If it's incorrect, search the full document text for a known bank name\n",
    "            statement_info['bank_name'] = 'Not Found'  # Default\n",
    "            for b_name in KNOWN_BANK_NAMES:\n",
    "                if b_name in document.text.lower():\n",
    "                    statement_info['bank_name'] = b_name.title()\n",
    "                    break\n",
    "        else:\n",
    "            # The extracted name seems valid, so we'll use it\n",
    "            statement_info['bank_name'] = extracted_bank_name\n",
    "    else:\n",
    "        # The model didn't find a bank_name, so fall back to searching the text\n",
    "        statement_info['bank_name'] = 'Not Found'  # Default\n",
    "        for b_name in KNOWN_BANK_NAMES:\n",
    "            if b_name in document.text.lower():\n",
    "                statement_info['bank_name'] = b_name.title()\n",
    "                break\n",
    "\n",
    "    # Hardcoding names is a good practical solution for a personal script\n",
    "    statement_info['primary_client_name'] = \"MOHIT AGGARWAL\"\n",
    "    statement_info['all_cardholders'] = [\"MOHIT AGGARWAL\", \"HIMANI SOOD\"]\n",
    "    \n",
    "    return statement_info, entities_by_type\n",
    "\n",
    "\n",
    "\n",
    "def parse_table_items(entities_by_type: Dict, document_text: str, all_cardholders: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse table_item entities into transaction records, associating each with the correct cardholder.\n",
    "    \"\"\"\n",
    "    if 'table_item' not in entities_by_type:\n",
    "        return []\n",
    "    \n",
    "    transactions = []\n",
    "    current_cardholder = \"Unknown\"  # Start with a default value\n",
    "    \n",
    "    print(f\"Found {len(entities_by_type['table_item'])} table items\")\n",
    "    \n",
    "    for i, table_item in enumerate(entities_by_type['table_item']):\n",
    "        raw_text = extract_entity_text(table_item, document_text)\n",
    "        \n",
    "        # Check if the raw text indicates a change in the cardholder context.\n",
    "        # The document processor often groups section headers with the first transaction.\n",
    "        for name in all_cardholders:\n",
    "            if name in raw_text:\n",
    "                current_cardholder = name\n",
    "                break\n",
    "        \n",
    "        transaction = {\n",
    "            'item_id': i,\n",
    "            'cardholder': current_cardholder,  # Add the tracked cardholder to the record\n",
    "            'raw_text': raw_text\n",
    "        }\n",
    "        \n",
    "        # Extract properties from each table item\n",
    "        if table_item.properties:\n",
    "            for prop in table_item.properties:\n",
    "                prop_type = prop.type_\n",
    "                prop_value = extract_entity_text(prop, document_text)\n",
    "                transaction[prop_type] = prop_value\n",
    "        \n",
    "        transactions.append(transaction)\n",
    "    \n",
    "    return transactions\n",
    "\n",
    "# Main execution code\n",
    "def analyze_and_extract_transactions(document):\n",
    "    \"\"\"Main function to analyze and extract transactions\"\"\"\n",
    "    \n",
    "    print(\"=== BANK STATEMENT ANALYSIS ===\")\n",
    "    \n",
    "    # Parse basic statement info\n",
    "    statement_info, entities_by_type = parse_bank_statement(document)\n",
    "    \n",
    "    print(\"\\n=== STATEMENT INFO ===\")\n",
    "    for key, value in statement_info.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "        \n",
    "    # Extract transactions, passing in the list of known cardholders\n",
    "    print(f\"\\n=== EXTRACTING TRANSACTIONS ===\")\n",
    "    transactions = parse_table_items(entities_by_type, document.text, statement_info['all_cardholders'])\n",
    "    \n",
    "    if transactions:\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(transactions)\n",
    "        \n",
    "        # Add the bank_name from the statement_info to every transaction row\n",
    "        df['bank_name'] = statement_info.get('bank_name', 'N/A')\n",
    "        \n",
    "        # Reorder columns to bring important info to the front\n",
    "        desired_order = ['bank_name', 'cardholder', 'item_id'] + [col for col in df.columns if col not in ['bank_name', 'cardholder', 'item_id']]\n",
    "        df = df[desired_order]\n",
    "\n",
    "        print(f\"\\n=== TRANSACTION SUMMARY ===\")\n",
    "        print(f\"Total transactions found: {len(df)}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Display the data\n",
    "        print(f\"\\n=== TRANSACTION DATA === \")\n",
    "        display(df)\n",
    "        \n",
    "        # Save to CSV\n",
    "        df.to_csv(\"bank_transactions_updated.csv\", index=False)\n",
    "        print(f\"\\n💾 Transactions saved to 'bank_transactions_updated.csv'\")\n",
    "        \n",
    "        return df, statement_info\n",
    "    else:\n",
    "        print(\"❌ No transactions extracted\")\n",
    "        return None, statement_info\n",
    "\n",
    "# To run the updated code, ensure you have the 'result' object from the API call\n",
    "# and then execute the following line:\n",
    "#\n",
    "# df, info = analyze_and_extract_transactions(result.document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58ef89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process extracted data\n",
    "def preprocess_transactions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses a transaction DataFrame by cleaning, coalescing, renaming,\n",
    "    and filtering out records with no amount.\n",
    "\n",
    "    Args:\n",
    "        df: The input DataFrame with raw transaction data.\n",
    "\n",
    "    Returns:\n",
    "        A cleaned DataFrame with a simplified schema.\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    processed_df = df.copy()\n",
    "\n",
    "    # 1. Remove 'item_id' and 'raw_text' columns\n",
    "    processed_df.drop(columns=['item_id', 'raw_text'], inplace=True)\n",
    "\n",
    "    # 2. Coalesce description columns\n",
    "    processed_df['table_item/transaction_withdrawal_description'] = processed_df['table_item/transaction_withdrawal_description'].fillna(\n",
    "        processed_df['table_item/transaction_deposit_description']\n",
    "    )\n",
    "\n",
    "    # 3. Coalesce amount columns\n",
    "    processed_df['table_item/transaction_withdrawal'] = processed_df['table_item/transaction_withdrawal'].fillna(\n",
    "        processed_df['table_item/transaction_deposit']\n",
    "    )\n",
    "    \n",
    "    # Coalesce date columns for completeness\n",
    "    processed_df['table_item/transaction_withdrawal_date'] = processed_df['table_item/transaction_withdrawal_date'].fillna(\n",
    "        processed_df['table_item/transaction_deposit_date']\n",
    "    )\n",
    "\n",
    "    # 4. Rename the primary columns\n",
    "    rename_map = {\n",
    "        'table_item/transaction_withdrawal_date': 'transaction_date',\n",
    "        'table_item/transaction_withdrawal_description': 'description',\n",
    "        'table_item/transaction_withdrawal': 'amount'\n",
    "    }\n",
    "    processed_df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    # 5. Drop the now-redundant original deposit columns\n",
    "    processed_df.drop(columns=[\n",
    "        'table_item/transaction_deposit_date',\n",
    "        'table_item/transaction_deposit_description',\n",
    "        'table_item/transaction_deposit'\n",
    "    ], inplace=True)\n",
    "\n",
    "    # 6. Drop records where the final 'amount' is missing\n",
    "    processed_df.dropna(subset=['amount'], inplace=True)\n",
    "\n",
    "    # 7. Drop records where the amount is $0.00\n",
    "    zero_values = ['$0.00', '+$0.00']\n",
    "    processed_df = processed_df[~processed_df['amount'].isin(zero_values)]\n",
    "\n",
    "    # 8. Standardize transaction_date format\n",
    "    def standardize_date(date_str):\n",
    "        if pd.isna(date_str) or date_str == '':\n",
    "            return None\n",
    "        \n",
    "        date_str = str(date_str).strip()\n",
    "        current_year = pd.Timestamp.now().year\n",
    "        \n",
    "        # Try different date formats\n",
    "        date_formats = [\n",
    "            '%m/%d/%Y',    # MM/DD/YYYY\n",
    "            '%m-%d-%Y',    # MM-DD-YYYY  \n",
    "            '%Y-%m-%d',    # YYYY-MM-DD\n",
    "            '%b %d',       # Jun 25, Jul 7 (abbreviated month, no year)\n",
    "            '%B %d',       # June 25, July 7 (full month, no year)\n",
    "            '%b %d, %Y',   # Jun 25, 2024 (abbreviated month with year)\n",
    "            '%B %d, %Y',   # June 25, 2024 (full month with year)\n",
    "            '%d %b',       # 25 Jun (day first, abbreviated month)\n",
    "            '%d %B',       # 25 June (day first, full month)\n",
    "            '%d %b %Y',    # 25 Jun 2024 (day first with year)\n",
    "            '%d %B %Y',    # 25 June 2024 (day first with year)\n",
    "            '%m/%d',       # MM/DD (no year)\n",
    "            '%m-%d'        # MM-DD (no year)\n",
    "        ]\n",
    "        \n",
    "        for fmt in date_formats:\n",
    "            try:\n",
    "                if fmt in ['%m/%d', '%m-%d', '%b %d', '%B %d', '%d %b', '%d %B']:\n",
    "                    # Add current year for formats without year\n",
    "                    if fmt in ['%m/%d', '%m-%d']:\n",
    "                        parsed_date = pd.to_datetime(f\"{current_year}/{date_str}\", format=f'%Y/{fmt}')\n",
    "                    else:\n",
    "                        # For month name formats, append current year\n",
    "                        parsed_date = pd.to_datetime(f\"{date_str} {current_year}\", format=f'{fmt} %Y')\n",
    "                else:\n",
    "                    parsed_date = pd.to_datetime(date_str, format=fmt)\n",
    "                \n",
    "                # Return as YYYY-MM-DD string format\n",
    "                return parsed_date.strftime('%Y-%m-%d')\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "        \n",
    "        # Last resort - let pandas try to parse\n",
    "        try:\n",
    "            parsed_date = pd.to_datetime(date_str, errors='coerce')\n",
    "            if pd.notna(parsed_date):\n",
    "                return parsed_date.strftime('%Y-%m-%d')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # Apply date standardization\n",
    "    processed_df['transaction_date'] = processed_df['transaction_date'].apply(standardize_date)\n",
    "    \n",
    "    # Drop records with invalid dates\n",
    "    processed_df.dropna(subset=['transaction_date'], inplace=True)\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c67f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN BATCH PROCESSING LOGIC\n",
    "# ==============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Processes all PDF statements in a folder, combines the cleaned data,\n",
    "    and exports it to a single CSV file.\n",
    "    \"\"\"\n",
    "    # --- CONFIGURATION ---\n",
    "    project_id = \"gen-lang-client-0299904904\"\n",
    "    location = \"us\"\n",
    "    processor_id = \"bf2685d686b2d8db\"\n",
    "    # ❗ IMPORTANT: Set this to the path of your statements folder\n",
    "    statements_folder = \"statements\" \n",
    "    temp_folder = \"temp\"\n",
    "    output_csv_path = os.path.join(temp_folder, \"data.csv\")\n",
    "\n",
    "    # Create temp folder if it doesn't exist\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "    print(f\"Temp folder '{temp_folder}' ready for output\")\n",
    "    \n",
    "    # --- PROCESSING ---\n",
    "    all_cleaned_dfs = []\n",
    "\n",
    "    print(f\"🚀 Starting batch processing for files in '{statements_folder}'...\")\n",
    "\n",
    "    # Get a list of all files in the directory\n",
    "    try:\n",
    "        file_names = os.listdir(statements_folder)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: The directory '{statements_folder}' was not found. Please check the path.\")\n",
    "        return\n",
    "\n",
    "    for file_name in file_names:\n",
    "        # Process only PDF files, skipping others (like .DS_Store on macOS)\n",
    "        if file_name.lower().endswith(\".pdf\"):\n",
    "            file_path = os.path.join(statements_folder, file_name)\n",
    "            print(f\"\\n📄 Processing file: {file_name}\")\n",
    "\n",
    "            # 1. Call Document AI API\n",
    "            document = process_document_sample(\n",
    "                project_id=project_id,\n",
    "                location=location,\n",
    "                processor_id=processor_id,\n",
    "                file_path=file_path,\n",
    "                mime_type=\"application/pdf\"\n",
    "                # Note: processor_version_id is omitted to use the latest default version\n",
    "            )\n",
    "\n",
    "            if document:\n",
    "                # 2. Extract transactions from the result\n",
    "                df, info = analyze_and_extract_transactions(document)\n",
    "\n",
    "                if df is not None and not df.empty:\n",
    "                    # 3. Preprocess and clean the DataFrame\n",
    "                    cleaned_df = preprocess_transactions(df)\n",
    "                    all_cleaned_dfs.append(cleaned_df)\n",
    "                    print(f\"✅ Successfully cleaned and added {len(cleaned_df)} transactions from {file_name}.\")\n",
    "                else:\n",
    "                    print(f\"⚠️ No transactions were extracted from {file_name}.\")\n",
    "    \n",
    "    # --- FINAL EXPORT ---\n",
    "    if all_cleaned_dfs:\n",
    "        # Combine all the individual cleaned DataFrames into one\n",
    "        final_df = pd.concat(all_cleaned_dfs, ignore_index=True)\n",
    "        \n",
    "        # Export the final combined DataFrame to a CSV file\n",
    "        final_df.to_csv(output_csv_path, index=False)\n",
    "        \n",
    "        print(\"\\n===================================================\")\n",
    "        print(f\"🎉 Batch processing complete!\")\n",
    "        print(f\"Total transactions processed: {len(final_df)}\")\n",
    "        print(f\"💾 Combined data saved to '{output_csv_path}'\")\n",
    "        print(\"===================================================\")\n",
    "    else:\n",
    "        print(\"\\n⏹️ No transactions were processed or found in any of the files.\")\n",
    "\n",
    "# --- RUN THE SCRIPT ---\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
